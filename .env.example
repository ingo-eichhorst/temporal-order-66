# ============================================================================
# REQUIRED: LM Studio Configuration
# ============================================================================
# Ensure LM Studio is running with the local server enabled.
# Default LM Studio API is available at http://localhost:1234/v1
LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1
# Optional; LM Studio does not require a real key but the client expects one.
LM_STUDIO_API_KEY=lm-studio
# Model name loaded in LM Studio
LM_STUDIO_MODEL=google/gemma-3-1b

# ============================================================================
# OPTIONAL: Langfuse Observability Configuration
# ============================================================================
# Langfuse runs locally in Docker (http://localhost:3000)
# To enable LLM tracing:
#   1. Start the system: docker-compose up -d
#   2. Open http://localhost:3000 and create an account (stays local)
#   3. Generate API keys from Settings â†’ API Keys
#   4. Uncomment and add the keys below
#   5. Restart agents: docker-compose restart agent-a agent-b
#
# LANGFUSE_PUBLIC_KEY=pk-lf-your-generated-key-from-local-langfuse
# LANGFUSE_SECRET_KEY=sk-lf-your-generated-key-from-local-langfuse

# ============================================================================
# NOTE: The following are configured in docker-compose.yml
# ============================================================================
# You don't need to set these in .env:
# - LANGFUSE_HOST=http://langfuse:3000
# - TEMPORAL_SERVER=temporal:7233
# - TEMPORAL_NAMESPACE=a2a-demo
# - AGENT_A_URL=http://agent-a:8081
# - AGENT_B_URL=http://agent-b:8080
# - PORT (8080 for agent-b, 8081 for agent-a)
# - DEMO_DRIVER_PORT=8082
